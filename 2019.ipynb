{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: '2019 Race Analysis'\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    header-includes: |\n",
    "      <style>.dataframe tbody th {white-space: nowrap}</style>\n",
    "execute:\n",
    "  enabled: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages that will be used throughout this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from processing import cleaners, io, util\n",
    "\n",
    "RACE_YEAR = 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then change a setting so we can see the results of variable assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'last_expr_or_assign'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes we have already run a scrapy spider to collect race data like so:\n",
    "\n",
    "```python\n",
    "from processing.scrapers import scrape_for_pandas\n",
    "scrape_for_pandas(RACE_YEAR)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, process, and clean split info scraped by the spider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data (as scraped by the spider) into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_info_raw = io.load_df_split_info_raw(RACE_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_info = cleaners.process_df_split_info(df_split_info_raw)\n",
    "\n",
    "# Debug\n",
    "# df_split_info\n",
    "# df_splits.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply some Leadville-specific processing to the split info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (present in 2021, gone in 2022, and never very useful)\n",
    "df_split_info.drop(index='Kick to Finish 0.8 Miles Left', errors='ignore', inplace=True)\n",
    "\n",
    "# Extract each aid station name from its 'label' string.\n",
    "# NOTE: This isn't really necessary, just aesthetic. Move to plotting script?\n",
    "df_split_info['name'] = df_split_info.index.to_series(\n",
    "  ).str.extract(r'Start to (.+)', expand=False,\n",
    "  # If the split name doesn't match the regex, just use as-is.\n",
    "  ).fillna(df_split_info.index.to_series())\n",
    "\n",
    "# Manually add cutoff info (found elsewhere on the web)\n",
    "df_split_info['cutoff_hr'] = [3.75, 6., 7.5, 9.5, 12., 14., None, 18., 21.25, 23., 26.5, 30.]\n",
    "\n",
    "df_split_info\n",
    "\n",
    "# Debug\n",
    "# df_splits.dtypes\n",
    "# type(df_splits['cutoff_hr'].iloc[6])  # np.float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) write cleaned split info to file\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# TODO: implement this\n",
    "# io.save_df_split_info(df_split_info, RACE_YEAR)\n",
    "\n",
    "# dir_out = settings.CLEAN_RACE_DATA_DIR\n",
    "dir_out = io.get_clean_race_data_dir(RACE_YEAR)\n",
    "if not os.path.exists(dir_out):\n",
    "  os.makedirs(dir_out)\n",
    "\n",
    "# df_split_info.to_csv(os.path.join(dir_out, settings.SPLIT_INFO_FNAME),\n",
    "df_split_info.to_csv(os.path.join(dir_out, io.SPLIT_INFO_FNAME),\n",
    "  columns=['name', 'distance_mi', 'cutoff_hr'],\n",
    "  index=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load, process, and clean the athlete split data scraped by the spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Athlinks spider saves split data for each athlete. Load all such split data from the spider-produced `json` file into a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_times_raw = io.load_df_split_times_raw(RACE_YEAR)\n",
    "\n",
    "util.df_td_fmt(df_split_times_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the columns (athletes) are presented in the order the Athlinks website spider returned them. This isn't particularly useful for spotting outliers or unreasonable performances. It would be easier to spot bad data if athletes with similar performances were grouped together. To fix this, we can group the athletes based on the furthest split they each recorded, then sort athletes within each group by their split times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the split info that's been processed and cleaned\n",
    "# NOTE: no need to reload, I created it above.\n",
    "# df_split_info = io.load_df_split_info_clean(settings.CLEAN_RACE_DATA_DIR)\n",
    "# print(df_split_info_clean)\n",
    "\n",
    "# df_split_times = util.load_athlete_split_times(clean=False)\n",
    "# df_split_times = cleaners.sort_df_split_data(\n",
    "#   io.load_df_split_times_raw(settings.RAW_RACE_DATA_DIR),\n",
    "#   df_split_info_clean)\n",
    "\n",
    "df_split_times = cleaners.sort_df_split_data(df_split_times_raw, df_split_info)\n",
    "\n",
    "util.display_full_df(util.df_td_fmt(df_split_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Print to a file so\n",
    "# I can inspect the whole thing for abnormalities (alt+z to disable wrap).\n",
    "# Mostly looking for anomalous finish line splits when a runner DNFs and \n",
    "# returns to Leadville.\n",
    "# NOTE: Cannot view this in vscode if it goes over 10k characters wide.\n",
    "# Sublime text can do it.\n",
    "# util.full_df_to_file(\n",
    "#   util.df_td_fmt(df_split_times),\n",
    "#   os.path.join(settings.RAW_RACE_DATA_DIR, 'df_all.txt')\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove wonky runner data, as determined by manual inspection.\n",
    "\n",
    "Viewing the data this way highlights erroneous split data. For example, look at the first column, containing the splits for Charles Corfield. The uncleaned split data suggests that Charles won the entire race, because his finish line split is the fastest. But looking more closely, it seems he dropped out of the race after the split labeled `56.5mi_Start to Hope Pass In`, and his bib triggered the sensor at the finish line when he came to watch the 2019 champion, Ryan Smith, bring it home. Something similar seemed to happen with Vineer Bhansali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.df_td_fmt(\n",
    "  df_split_times[['Charles Corfield', 'Ryan Smith', 'Vineer Bhansali']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To cleanse the data of these erroneous splits, we just need to set them to a null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_times.loc['Full Course', 'Charles Corfield'] = pd.NA\n",
    "df_split_times.loc['Full Course', 'Vineer Bhansali'] = pd.NA\n",
    "\n",
    "util.df_td_fmt(\n",
    "  df_split_times[['Charles Corfield', 'Ryan Smith', 'Vineer Bhansali']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another type of error can happen when a split is triggered erroneously early, for whatever reason. This type of erroneous data doesn't necessarily jump out upon visual inspection. One way to check for it is to see if any athletes data includes a negative point-to-point time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split_times.diff().iloc[1:].apply(lambda col: (col.dt.total_seconds() < 0).any())\n",
    "# df_split_times.diff().iloc[1:].apply(lambda col: col[~col.isnull()].dt.total_seconds() < 0).all()\n",
    "# df_split_times.diff().iloc[1:].apply(lambda col: (col.dt.total_seconds() > 0))\n",
    "# df_split_times.diff().iloc[1:].apply(lambda col: (col.dt.total_seconds() > 0).all())\n",
    "# df_split_times.apply(lambda col: col.is_monotonic_increasing)\n",
    "\n",
    "# series_athlete_has_negative_timedelta = df_split_times.diff().apply(\n",
    "#   lambda col: (col.dt.total_seconds() < 0).any())\n",
    "\n",
    "series_athlete_has_negative_timedelta = df_split_times.apply(\n",
    "  lambda col: (col[col.notnull()].dt.total_seconds().diff() < 0).any())\n",
    "\n",
    "# series_athlete_has_negative_timedelta = df_split_times.apply(\n",
    "#   lambda col: col.notnull()\n",
    "# )\n",
    "\n",
    "util.df_td_fmt(\n",
    "  df_split_times.loc[:, series_athlete_has_negative_timedelta]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not immediately clear what is going on with the split data for either of these athletes.\n",
    "\n",
    "In Willie's case, the erroneously negative elapsed time comes between the splits at 76.9 miles and 87.8 miles. But even if we discarded the split data at 87.8 miles as an obvious misfire, no one could cover the distance from 76.9 miles to the finish in such a short time. But is the error at 76.9 miles or the finish line? Both?\n",
    "\n",
    "It's impossible to tell which split data is valid or invalid without more context. Let's look at athletes with similar finish times to each athlete with bad data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = df_split_times.columns.get_loc('Willie Stewart')\n",
    "util.df_td_fmt(\n",
    "  df_split_times.iloc[:, ix-5:ix+5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow now I have no idea what data is valid.\n",
    "\n",
    "* It seems likely that Willie Stewart DNFed at 76.9. I just doubt anyone could cover 71.1 -> 100 in 3ish hours,\n",
    "  so it seems very unlikely that the split at 76.9 is a misfire.\n",
    "* Chavet Breslin probably DNFed at 76.9 and recorded an erroneous finish.\n",
    "* Harry Harcrow and Jared Conlin seem to have DNFed at 71.1 but recorded a finish line split near each other.\n",
    "* Tim Finocchio seemed sketchy at first with how much ground he made up in the later splits, but I\n",
    "  cannot say anything without busting out the statistics.\n",
    "\n",
    "Let's cleanse the more likely errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_times.loc['87.8mi_Start to May Queen In': 'Full Course', 'Willie Stewart'] = pd.NA\n",
    "df_split_times.loc['Full Course', 'Chavet Breslin'] = pd.NA\n",
    "df_split_times.loc['Full Course', 'Harry Harcrow'] = pd.NA\n",
    "df_split_times.loc['Full Course', 'Jared Conlin'] = pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the context around Max's finish split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = df_split_times.columns.get_loc('Max Fulton')\n",
    "util.df_td_fmt(\n",
    "  df_split_times.iloc[:, ix-5:ix+5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediately, it looks like the split at 43.5 misfired. I think his chip didn't fire on the way out,\n",
    "but it did fire on the way in. Let's swap those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split_times.loc['56.5mi_Start to Hope Pass In', 'Max Fulton'] = df_split_times.loc['43.5mi_Start to Hope Pass Out', 'Max Fulton']\n",
    "df_split_times.loc['43.5mi_Start to Hope Pass Out', 'Max Fulton'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Use statistical methods or reasonable assumptions to \n",
    "# programmatically flag sketchy split times. Single splits that are\n",
    "# anomalously fast are invisible to me when I'm visually checking the\n",
    "# output.\n",
    "\n",
    "# TODO: Move the functionality below somewhere else - it represents insights\n",
    "#       that can only be gained after looking at the results of the\n",
    "#       script so far.\n",
    "\n",
    "# TODO: Make this programmatic if it needs to be done more than once.\n",
    "if RACE_YEAR == 2022:\n",
    "  # Erroneous finish line split after a dnf\n",
    "  df_split_times.loc['Full Course', 'Mickey Davis'] = pd.NA\n",
    "\n",
    "  df_split_times.loc['76.9mi_Start to Outward Bound In', 'Timothy Weng'] = pd.NA\n",
    "\n",
    "  # He would have had to run an unreasonably fast time to get to the next split. \n",
    "  df_split_times.loc['29.3mi_Start to Half Pipe Out', 'Adrian Macdonald'] = pd.NA\n",
    "\n",
    "  # chip reported a negative time from last split\n",
    "  df_split_times.loc['43.5mi_Start to Hope Pass Out', 'Kris Rugloski'] = pd.NA\n",
    "\n",
    "  # 05:10:20 split would put him in first. Hmm.\n",
    "  # Next two splits are pretty fast for his eventual finish time too.\n",
    "  # Could have been a wonky chip or an aggressive strategy, idk.\n",
    "  # df_split_times.loc['37.9mi_Start to Twin Lakes Out', 'Michael Nanaszko'] = pd.NA\n",
    "\n",
    "elif RACE_YEAR == 2021:\n",
    "  # chip reported a negative time from last split\n",
    "  df_split_times.loc['37.9mi_Start to Twin Lakes Out', 'Matthew DuBois'] = pd.NA\n",
    "  # TODO: finish checking.\n",
    "\n",
    "df_split_times = cleaners.sort_df_split_data(df_split_times, df_split_info)\n",
    "\n",
    "util.display_full_df(util.df_td_fmt(df_split_times))\n",
    "\n",
    "# series_athlete_has_negative_timedelta = df_split_times.apply(\n",
    "#   lambda col: (col[col.notnull()].dt.total_seconds().diff() < 0).any())\n",
    "# df_split_times.loc[:, series_athlete_has_negative_timedelta]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) save inferences for future use\n",
    "\n",
    "```python\n",
    "# TODO: Implement?\n",
    "# util.save_athlete_split_times(df_all_athlete_split_times, clean=True)\n",
    "\n",
    "dir_out = settings.CLEAN_RACE_DATA_DIR\n",
    "if not os.path.exists(settings.CLEAN_RACE_DATA_DIR):\n",
    "  os.makedirs(settings.CLEAN_RACE_DATA_DIR)\n",
    "io.df_td_to_csv(df_split_times, \n",
    "  os.path.join(settings.CLEAN_RACE_DATA_DIR, settings.SPLIT_SECS_FNAME))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "232075263f2bbb1b1b9e2fa382049cbe7951d10107be8099e345904b48a1a09c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
